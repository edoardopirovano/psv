// MDP-DTMC model implementing collective decision making algorithm of:
// F. Saffre and A. Simaitis. Host Selection through Collective Decision
// ACM Transactions on Autonomous and Adaptive Systems (TAAS). 2011. 
//
// In contrast with original (DTMC) model, some agents are allowed to 
// make a decision whether to explore or communicate modeled by non-determinism.
// 
// Model has to be built using PRISM preprocessor (http://www.prismmodelchecker.org/prismpp/)
// using the following command: prismpp cdm_mdp-dtmc.pp <N> <D> <K> <L> > cmd_mdp-dtmc.pm, where 
// <N> - number of agents,
// <D> - number of deterministic agents,
// <K> - number of hosting sites,
// <L> - number of confidence levels.
//
// Hosting site qualities and other constant model parameters should
// be adjusted directly in the model file.
//
// Aistis Simaitis 23/06/11 


smg

// number of agents
const int N = 3;

// number of sites
const int K = 3;

// number of confidence levels
const int L = 2;

// model parameters
const double Pexp;
const double eta;
const double gamma;
const double lambda;

// quality of the sites
const double Q1;
const double Q2;
const double Q3;

// confidence levels of agents
global confidence1 : [1..L];
global confidence2 : [1..L];
global confidence3 : [1..L];

// site preferences of agents
global preference1 : [0..K] init 0;
global preference2 : [0..K] init 0;
global preference3 : [0..K] init 0;


// scheduling variable
global sched : [0..N];

// scheduler module
module player0
	[] sched = 0 -> 1/N : (sched'=1)
		      + 1/N : (sched'=2)
		      + 1/N : (sched'=3)
;

	
endmodule


// non-deterministic agent definitions

// deterministic agent definitions
module player1

	// taking action
	[] sched=1 -> 0 : true
		// - exploring with probability Pexp
			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference1=0?1:Pexp)/K * Pswitch1_1 : (preference1'=1) & (confidence1'=1) & (sched'=0)
			  + (preference1=0?1:Pexp)/K * (1-Pswitch1_1) : (sched'=0)

			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference1=0?1:Pexp)/K * Pswitch1_2 : (preference1'=2) & (confidence1'=1) & (sched'=0)
			  + (preference1=0?1:Pexp)/K * (1-Pswitch1_2) : (sched'=0)

			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference1=0?1:Pexp)/K * Pswitch1_3 : (preference1'=3) & (confidence1'=1) & (sched'=0)
			  + (preference1=0?1:Pexp)/K * (1-Pswitch1_3) : (sched'=0)

		// - communicating with other agents in the same site with probability 1-Pexp

			// -- trying to communicate with agent
			  + (preference1=0?0:(1-Pexp)) * Pmeet_p1 * (preference1=preference2?1:0) : (confidence1'=inc_conf1) & (confidence2'=inc_conf2) & (sched'=0) // same site
			  + (preference1=0?0:(1-Pexp)) * Pmeet_p1 * (preference1=preference2?0:1) * Pwin1_2 : (confidence1'=inc_conf1) & (preference2'=preference1) & (confidence2'=1) & (sched'=0) // win
			  + (preference1=0?0:(1-Pexp)) * Pmeet_p1 * (preference1=preference2?0:1) * (1-Pwin1_2) : (confidence1'=1) & (preference1'=preference2) & (confidence2'=inc_conf2) & (sched'=0) // lose
			
						// -- trying to communicate with agent
			  + (preference1=0?0:(1-Pexp)) * Pmeet_p1 * (preference1=preference3?1:0) : (confidence1'=inc_conf1) & (confidence3'=inc_conf3) & (sched'=0) // same site
			  + (preference1=0?0:(1-Pexp)) * Pmeet_p1 * (preference1=preference3?0:1) * Pwin1_3 : (confidence1'=inc_conf1) & (preference3'=preference1) & (confidence3'=1) & (sched'=0) // win
			  + (preference1=0?0:(1-Pexp)) * Pmeet_p1 * (preference1=preference3?0:1) * (1-Pwin1_3) : (confidence1'=1) & (preference1'=preference3) & (confidence3'=inc_conf3) & (sched'=0) // lose
			
			;
endmodule

module player2

	// taking action
	[] sched=2 -> 0 : true
		// - exploring with probability Pexp
			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference2=0?1:Pexp)/K * Pswitch2_1 : (preference2'=1) & (confidence2'=1) & (sched'=0)
			  + (preference2=0?1:Pexp)/K * (1-Pswitch2_1) : (sched'=0)

			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference2=0?1:Pexp)/K * Pswitch2_2 : (preference2'=2) & (confidence2'=1) & (sched'=0)
			  + (preference2=0?1:Pexp)/K * (1-Pswitch2_2) : (sched'=0)

			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference2=0?1:Pexp)/K * Pswitch2_3 : (preference2'=3) & (confidence2'=1) & (sched'=0)
			  + (preference2=0?1:Pexp)/K * (1-Pswitch2_3) : (sched'=0)

		// - communicating with other agents in the same site with probability 1-Pexp

			// -- trying to communicate with agent
			  + (preference2=0?0:(1-Pexp)) * Pmeet_p2 * (preference2=preference1?1:0) : (confidence2'=inc_conf2) & (confidence1'=inc_conf1) & (sched'=0) // same site
			  + (preference2=0?0:(1-Pexp)) * Pmeet_p2 * (preference2=preference1?0:1) * Pwin2_1 : (confidence2'=inc_conf2) & (preference1'=preference2) & (confidence1'=1) & (sched'=0) // win
			  + (preference2=0?0:(1-Pexp)) * Pmeet_p2 * (preference2=preference1?0:1) * (1-Pwin2_1) : (confidence2'=1) & (preference2'=preference1) & (confidence1'=inc_conf1) & (sched'=0) // lose
			
			// -- trying to communicate with agent
			  + (preference2=0?0:(1-Pexp)) * Pmeet_p2 * (preference2=preference3?1:0) : (confidence2'=inc_conf2) & (confidence3'=inc_conf3) & (sched'=0) // same site
			  + (preference2=0?0:(1-Pexp)) * Pmeet_p2 * (preference2=preference3?0:1) * Pwin2_3 : (confidence2'=inc_conf2) & (preference3'=preference2) & (confidence3'=1) & (sched'=0) // win
			  + (preference2=0?0:(1-Pexp)) * Pmeet_p2 * (preference2=preference3?0:1) * (1-Pwin2_3) : (confidence2'=1) & (preference2'=preference3) & (confidence3'=inc_conf3) & (sched'=0) // lose
			
			;
endmodule

module player3

	// taking action
	[] sched=3 -> 0 : true
		// - exploring with probability Pexp
			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference3=0?1:Pexp)/K * Pswitch3_1 : (preference3'=1) & (confidence3'=1) & (sched'=0)
			  + (preference3=0?1:Pexp)/K * (1-Pswitch3_1) : (sched'=0)

			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference3=0?1:Pexp)/K * Pswitch3_2 : (preference3'=2) & (confidence3'=1) & (sched'=0)
			  + (preference3=0?1:Pexp)/K * (1-Pswitch3_2) : (sched'=0)

			// -- evaluating site and changing preference with probability Pswitchxy
			  + (preference3=0?1:Pexp)/K * Pswitch3_3 : (preference3'=3) & (confidence3'=1) & (sched'=0)
			  + (preference3=0?1:Pexp)/K * (1-Pswitch3_3) : (sched'=0)

		// - communicating with other agents in the same site with probability 1-Pexp

			// -- trying to communicate with agent
			  + (preference3=0?0:(1-Pexp)) * Pmeet_p3 * (preference3=preference1?1:0) : (confidence3'=inc_conf3) & (confidence1'=inc_conf1) & (sched'=0) // same site
			  + (preference3=0?0:(1-Pexp)) * Pmeet_p3 * (preference3=preference1?0:1) * Pwin3_1 : (confidence3'=inc_conf3) & (preference1'=preference3) & (confidence1'=1) & (sched'=0) // win
			  + (preference3=0?0:(1-Pexp)) * Pmeet_p3 * (preference3=preference1?0:1) * (1-Pwin3_1) : (confidence3'=1) & (preference3'=preference1) & (confidence1'=inc_conf1) & (sched'=0) // lose
			
			// -- trying to communicate with agent
			  + (preference3=0?0:(1-Pexp)) * Pmeet_p3 * (preference3=preference2?1:0) : (confidence3'=inc_conf3) & (confidence2'=inc_conf2) & (sched'=0) // same site
			  + (preference3=0?0:(1-Pexp)) * Pmeet_p3 * (preference3=preference2?0:1) * Pwin3_2 : (confidence3'=inc_conf3) & (preference2'=preference3) & (confidence2'=1) & (sched'=0) // win
			  + (preference3=0?0:(1-Pexp)) * Pmeet_p3 * (preference3=preference2?0:1) * (1-Pwin3_2) : (confidence3'=1) & (preference3'=preference2) & (confidence2'=inc_conf2) & (sched'=0) // lose
			
;
endmodule



// formulae to increase agents' confidence levels
	
	formula inc_conf1 = confidence1=L ? L : (confidence1+1);
	formula inc_conf2 = confidence2=L ? L : (confidence2+1);
	formula inc_conf3 = confidence3=L ? L : (confidence3+1);

// formulae to compute probabilities of agents to meet

	// probability for agent to meet another agent independent of its location
	formula Pmeet_p1 = 1/(N-1);
	formula Pmeet_p2 = 1/(N-1);
	formula Pmeet_p3 = 1/(N-1);


// formulae to get qualities of agents' preferred sites
	formula Q_p1 =  preference1=1 ? Q1 : ( preference1=1 ? Q2 : (Q3) ) ;
	formula Q_p2 =  preference2=1 ? Q1 : ( preference2=1 ? Q2 : (Q3) ) ;
	formula Q_p3 =  preference3=1 ? Q1 : ( preference3=1 ? Q2 : (Q3) ) ;

// formulae for evaluating the sites (Pswitchij = prob of to switch from size i to site j).
	
	formula Pswitch1_1 = preference1=0 ? 1 : (preference1=1 ? 0 : pow(Q1, eta) / (pow(Q1, eta) + pow(Q_p1, eta)));	
	formula Pswitch1_2 = preference1=0 ? 1 : (preference1=2 ? 0 : pow(Q2, eta) / (pow(Q2, eta) + pow(Q_p1, eta)));	
	formula Pswitch1_3 = preference1=0 ? 1 : (preference1=3 ? 0 : pow(Q3, eta) / (pow(Q3, eta) + pow(Q_p1, eta)));	

	formula Pswitch2_1 = preference2=0 ? 1 : (preference2=1 ? 0 : pow(Q1, eta) / (pow(Q1, eta) + pow(Q_p2, eta)));	
	formula Pswitch2_2 = preference2=0 ? 1 : (preference2=2 ? 0 : pow(Q2, eta) / (pow(Q2, eta) + pow(Q_p2, eta)));	
	formula Pswitch2_3 = preference2=0 ? 1 : (preference2=3 ? 0 : pow(Q3, eta) / (pow(Q3, eta) + pow(Q_p2, eta)));	

	formula Pswitch3_1 = preference3=0 ? 1 : (preference3=1 ? 0 : pow(Q1, eta) / (pow(Q1, eta) + pow(Q_p3, eta)));	
	formula Pswitch3_2 = preference3=0 ? 1 : (preference3=2 ? 0 : pow(Q2, eta) / (pow(Q2, eta) + pow(Q_p3, eta)));	
	formula Pswitch3_3 = preference3=0 ? 1 : (preference3=3 ? 0 : pow(Q3, eta) / (pow(Q3, eta) + pow(Q_p3, eta)));	


// formulae for conducting tournaments

	formula Pwin1_2 = (preference2=0?1:(preference1=0?0:((pow(Q_p1, lambda) * pow(confidence1, gamma)) / 
		((pow(Q_p1, lambda) * pow(confidence1, gamma))+(pow(Q_p2, lambda) * pow(confidence2, gamma))))));
	formula Pwin1_3 = (preference3=0?1:(preference1=0?0:((pow(Q_p1, lambda) * pow(confidence1, gamma)) / 
		((pow(Q_p1, lambda) * pow(confidence1, gamma))+(pow(Q_p3, lambda) * pow(confidence3, gamma))))));

	formula Pwin2_1 = 1-Pwin1_2;
	formula Pwin2_3 = (preference3=0?1:(preference2=0?0:((pow(Q_p2, lambda) * pow(confidence2, gamma)) / 
		((pow(Q_p2, lambda) * pow(confidence2, gamma))+(pow(Q_p3, lambda) * pow(confidence3, gamma))))));

	formula Pwin3_1 = 1-Pwin1_3;
	formula Pwin3_2 = 1-Pwin2_3;


// labeling states
	
// -- formulae to generate labels
	
	// agreement on site
	formula all_prefer_1 =  preference1=1 & preference2=1 & preference3=1 ;
	formula all_prefer_2 =  preference1=2 & preference2=2 & preference3=2 ;
	formula all_prefer_3 =  preference1=3 & preference2=3 & preference3=3 ;

	// compute total confidence
	formula total_confidence =  confidence1 + confidence2 + confidence3 ;
	
	// confidence measures
	formula all_max_conf = total_confidence/N = L;
	formula half_max_conf = (( confidence1=L?1:0 + confidence2=L?1:0 + confidence3=L?1:0 )/N) >= 0.5;
		
// -- labels

	// agreement on particular sites
	label "all_prefer_1" = all_prefer_1;
	label "all_prefer_2" = all_prefer_2;
	label "all_prefer_3" = all_prefer_3;

	// all agents have max confidence
	label "all_max_conf" = all_max_conf;

	label "half_max_conf" = half_max_conf;	

	// agreement on a site
	label "decision_made" =  all_prefer_1 | all_prefer_2 | all_prefer_3 ;

// -- rewards

const int communication_cost = 15;
const int exploration_cost = 1;

// communication n costs

// communication d costs
rewards "dcomm1"
	[] sched=1 : (1-Pexp)*communication_cost;
endrewards
rewards "dcomm12"
	[] sched=1 : (1-Pexp)*communication_cost;
	[] sched=2 : (1-Pexp)*communication_cost;
endrewards
rewards "dcomm123"
	[] sched=1 : (1-Pexp)*communication_cost;
	[] sched=2 : (1-Pexp)*communication_cost;
	[] sched=3 : (1-Pexp)*communication_cost;
endrewards

// exploration n costs

// exploration d costs
rewards "dexpl1"
	[] sched=1 : Pexp*communication_cost;
endrewards
rewards "dexpl12"
	[] sched=1 : Pexp*communication_cost;
	[] sched=2 : Pexp*communication_cost;
endrewards
rewards "dexpl123"
	[] sched=1 : Pexp*communication_cost;
	[] sched=2 : Pexp*communication_cost;
	[] sched=3 : Pexp*communication_cost;
endrewards

// total n costs

// total d costs
rewards "dtot1"
	[] sched=1 : Pexp*exploration_cost + (1-Pexp)*communication_cost;
endrewards
rewards "dtot12"
	[] sched=1 : Pexp*exploration_cost + (1-Pexp)*communication_cost;
	[] sched=2 : Pexp*exploration_cost + (1-Pexp)*communication_cost;
endrewards
rewards "dtot123"
	[] sched=1 : Pexp*exploration_cost + (1-Pexp)*communication_cost;
	[] sched=2 : Pexp*exploration_cost + (1-Pexp)*communication_cost;
	[] sched=3 : Pexp*exploration_cost + (1-Pexp)*communication_cost;
endrewards
	
rewards "runtime"
	sched!=0 : 1;
endrewards




