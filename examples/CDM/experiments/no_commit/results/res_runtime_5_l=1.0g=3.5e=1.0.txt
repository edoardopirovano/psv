PRISM
=====

Version: 4.0.1.games
Date: Fri Oct 14 21:53:00 BST 2011
Hostname: qavbench.comlab
Command line: prism -ex examples/games/CDM/experiments/no_commit/cdm5032.smg examples/games/CDM/experiments/no_commit/props_runtime_5.pctl -const 'k=0,Pexp=0.5,Q1=1.0,Q2=0.5,Q3=0.25,gamma=3.5,eta=1.0,lambda=1.0'

Parsing model file "examples/games/CDM/experiments/no_commit/cdm5032.smg"...

Parsing properties file "examples/games/CDM/experiments/no_commit/props_runtime_5.pctl"...

5 properties:
(1) <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
(2) <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
(3) <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
(4) <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
(5) <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]

Model constants: Pexp=0.5,eta=1,gamma=3.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0

-------------------------------------------

Building model...

Computing reachable states...
 70638 100032
Reachable states exploration and model construction done in 4.351 secs.
Sorting reachable states list...

Time for model construction: 4.595 seconds.

Type:        SMG

States:      100032 (1 initial)
Transitions: 760430
Choices:     171747
Max/avg:     2/1.72

-------------------------------------------

Model checking: <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=3.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.804 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 2365 iterations and 131.567 seconds.
Computed an over-approximation of the solution (in 131 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1334 iterations and 70.952 seconds.
Expected reachability took 203.356 seconds.

Value in the initial state: 112.59560219741134

Time for model checking: 203.455 seconds.

Result: 112.59560219741134 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=3.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.636 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 920 iterations and 49.605 seconds.
Computed an over-approximation of the solution (in 49 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 566 iterations and 30.59 seconds.
Expected reachability took 80.894 seconds.

Value in the initial state: 44.06016063667212

Time for model checking: 80.921 seconds.

Result: 44.06016063667212 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=3.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.612 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 483 iterations and 26.661 seconds.
Computed an over-approximation of the solution (in 26 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 313 iterations and 16.922 seconds.
Expected reachability took 44.206 seconds.

Value in the initial state: 25.87671511606306

Time for model checking: 44.231 seconds.

Result: 25.87671511606306 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=3.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.608 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 281 iterations and 15.143 seconds.
Computed an over-approximation of the solution (in 15 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 191 iterations and 10.319 seconds.
Expected reachability took 26.081 seconds.

Value in the initial state: 18.046623844275945

Time for model checking: 26.099 seconds.

Result: 18.046623844275945 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=3.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.557 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 165 iterations and 8.997 seconds.
Computed an over-approximation of the solution (in 8 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 120 iterations and 6.408 seconds.
Expected reachability took 15.971 seconds.

Value in the initial state: 13.882174169583447

Time for model checking: 15.99 seconds.

Result: 13.882174169583447 (value in the initial state)

