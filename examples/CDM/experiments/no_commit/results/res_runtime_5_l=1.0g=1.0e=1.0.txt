PRISM
=====

Version: 4.0.1.games
Date: Fri Oct 14 21:28:12 BST 2011
Hostname: qavbench.comlab
Command line: prism -ex examples/games/CDM/experiments/no_commit/cdm5032.smg examples/games/CDM/experiments/no_commit/props_runtime_5.pctl -const 'k=0,Pexp=0.5,Q1=1.0,Q2=0.5,Q3=0.25,gamma=1.0,eta=1.0,lambda=1.0'

Parsing model file "examples/games/CDM/experiments/no_commit/cdm5032.smg"...

Parsing properties file "examples/games/CDM/experiments/no_commit/props_runtime_5.pctl"...

5 properties:
(1) <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
(2) <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
(3) <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
(4) <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
(5) <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]

Model constants: Pexp=0.5,eta=1,gamma=1,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0

-------------------------------------------

Building model...

Computing reachable states...
 86402 100032
Reachable states exploration and model construction done in 3.513 secs.
Sorting reachable states list...

Time for model construction: 3.726 seconds.

Type:        SMG

States:      100032 (1 initial)
Transitions: 760430
Choices:     171747
Max/avg:     2/1.72

-------------------------------------------

Model checking: <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=1,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.606 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 900 iterations and 50.37 seconds.
Computed an over-approximation of the solution (in 50 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 558 iterations and 37.968 seconds.
Expected reachability took 88.976 seconds.

Value in the initial state: 56.10764318263824

Time for model checking: 89.126 seconds.

Result: 56.10764318263824 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=1,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.978 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 445 iterations and 36.102 seconds.
Computed an over-approximation of the solution (in 36 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 292 iterations and 23.559 seconds.
Expected reachability took 60.748 seconds.

Value in the initial state: 31.54492946999948

Time for model checking: 60.849 seconds.

Result: 31.54492946999948 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=1,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.808 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 290 iterations and 24.454 seconds.
Computed an over-approximation of the solution (in 24 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 196 iterations and 15.752 seconds.
Expected reachability took 41.03 seconds.

Value in the initial state: 22.336358245702538

Time for model checking: 41.057 seconds.

Result: 22.336358245702538 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=1,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.803 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 209 iterations and 17.536 seconds.
Computed an over-approximation of the solution (in 17 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 145 iterations and 11.6 seconds.
Expected reachability took 29.953 seconds.

Value in the initial state: 17.37267921691388

Time for model checking: 29.981 seconds.

Result: 17.37267921691388 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=1,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.729 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 155 iterations and 13.028 seconds.
Computed an over-approximation of the solution (in 13 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 110 iterations and 9.115 seconds.
Expected reachability took 22.886 seconds.

Value in the initial state: 14.23711436800808

Time for model checking: 22.912 seconds.

Result: 14.23711436800808 (value in the initial state)

