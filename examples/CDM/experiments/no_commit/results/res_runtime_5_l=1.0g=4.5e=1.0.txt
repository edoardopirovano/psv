PRISM
=====

Version: 4.0.1.games
Date: Fri Oct 14 22:06:51 BST 2011
Hostname: qavbench.comlab
Command line: prism -ex examples/games/CDM/experiments/no_commit/cdm5032.smg examples/games/CDM/experiments/no_commit/props_runtime_5.pctl -const 'k=0,Pexp=0.5,Q1=1.0,Q2=0.5,Q3=0.25,gamma=4.5,eta=1.0,lambda=1.0'

Parsing model file "examples/games/CDM/experiments/no_commit/cdm5032.smg"...

Parsing properties file "examples/games/CDM/experiments/no_commit/props_runtime_5.pctl"...

5 properties:
(1) <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
(2) <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
(3) <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
(4) <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
(5) <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]

Model constants: Pexp=0.5,eta=1,gamma=4.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0

-------------------------------------------

Building model...

Computing reachable states...
 98537 100032
Reachable states exploration and model construction done in 3.061 secs.
Sorting reachable states list...

Time for model construction: 3.258 seconds.

Type:        SMG

States:      100032 (1 initial)
Transitions: 760430
Choices:     171747
Max/avg:     2/1.72

-------------------------------------------

Model checking: <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=4.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.508 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 4093 iterations and 194.264 seconds.
Computed an over-approximation of the solution (in 194 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 2168 iterations and 100.972 seconds.
Expected reachability took 295.77 seconds.

Value in the initial state: 189.27973774697108

Time for model checking: 295.844 seconds.

Result: 189.27973774697108 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=4.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.565 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1338 iterations and 62.19 seconds.
Computed an over-approximation of the solution (in 62 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 797 iterations and 36.311 seconds.
Expected reachability took 99.112 seconds.

Value in the initial state: 55.54524747873646

Time for model checking: 99.131 seconds.

Result: 55.54524747873646 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=4.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.54 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 619 iterations and 28.589 seconds.
Computed an over-approximation of the solution (in 28 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 393 iterations and 17.811 seconds.
Expected reachability took 46.947 seconds.

Value in the initial state: 28.3365977072288

Time for model checking: 46.964 seconds.

Result: 28.3365977072288 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=4.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.536 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 319 iterations and 14.671 seconds.
Computed an over-approximation of the solution (in 14 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 214 iterations and 9.614 seconds.
Expected reachability took 24.828 seconds.

Value in the initial state: 18.52355272223128

Time for model checking: 24.844 seconds.

Result: 18.52355272223128 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=4.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.487 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 171 iterations and 7.764 seconds.
Computed an over-approximation of the solution (in 7 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 124 iterations and 5.518 seconds.
Expected reachability took 13.777 seconds.

Value in the initial state: 13.862671051073587

Time for model checking: 13.793 seconds.

Result: 13.862671051073587 (value in the initial state)

