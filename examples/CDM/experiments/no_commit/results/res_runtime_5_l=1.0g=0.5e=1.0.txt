PRISM
=====

Version: 4.0.1.games
Date: Fri Oct 14 21:23:45 BST 2011
Hostname: qavbench.comlab
Command line: prism -ex examples/games/CDM/experiments/no_commit/cdm5032.smg examples/games/CDM/experiments/no_commit/props_runtime_5.pctl -const 'k=0,Pexp=0.5,Q1=1.0,Q2=0.5,Q3=0.25,gamma=0.5,eta=1.0,lambda=1.0'

Parsing model file "examples/games/CDM/experiments/no_commit/cdm5032.smg"...

Parsing properties file "examples/games/CDM/experiments/no_commit/props_runtime_5.pctl"...

5 properties:
(1) <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
(2) <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
(3) <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
(4) <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
(5) <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]

Model constants: Pexp=0.5,eta=1,gamma=0.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0

-------------------------------------------

Building model...

Computing reachable states...
 72119 100032
Reachable states exploration and model construction done in 4.205 secs.
Sorting reachable states list...

Time for model construction: 4.464 seconds.

Type:        SMG

States:      100032 (1 initial)
Transitions: 760430
Choices:     171747
Max/avg:     2/1.72

-------------------------------------------

Model checking: <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=0.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.841 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 845 iterations and 70.033 seconds.
Computed an over-approximation of the solution (in 70 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 527 iterations and 42.336 seconds.
Expected reachability took 113.249 seconds.

Value in the initial state: 54.18340793307564

Time for model checking: 113.361 seconds.

Result: 54.18340793307564 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=0.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.918 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 424 iterations and 35.518 seconds.
Computed an over-approximation of the solution (in 35 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 279 iterations and 22.446 seconds.
Expected reachability took 58.945 seconds.

Value in the initial state: 31.082358677937506

Time for model checking: 58.991 seconds.

Result: 31.082358677937506 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=0.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.807 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 281 iterations and 24.167 seconds.
Computed an over-approximation of the solution (in 24 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 190 iterations and 15.833 seconds.
Expected reachability took 40.823 seconds.

Value in the initial state: 22.274823050108722

Time for model checking: 40.849 seconds.

Result: 22.274823050108722 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=0.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.814 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 207 iterations and 17.882 seconds.
Computed an over-approximation of the solution (in 17 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 143 iterations and 11.924 seconds.
Expected reachability took 30.637 seconds.

Value in the initial state: 17.49615214210548

Time for model checking: 30.663 seconds.

Result: 17.49615214210548 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=0.5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.731 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 159 iterations and 10.049 seconds.
Computed an over-approximation of the solution (in 10 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 110 iterations and 5.981 seconds.
Expected reachability took 16.776 seconds.

Value in the initial state: 14.448345954809056

Time for model checking: 16.802 seconds.

Result: 14.448345954809056 (value in the initial state)

