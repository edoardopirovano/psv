PRISM
=====

Version: 4.0.1.games
Date: Fri Oct 14 21:16:59 BST 2011
Hostname: qavbench.comlab
Command line: prism -ex examples/games/CDM/experiments/no_commit/cdm5032.smg examples/games/CDM/experiments/no_commit/props_recovery_from_3_N5.pctl -const 'Pexp=0.5,Q1=1.0,Q2=0.5,Q3=0.25,gamma=2.0,eta=1.0,lambda=1.0,k=0'

Parsing model file "examples/games/CDM/experiments/no_commit/cdm5032.smg"...

Parsing properties file "examples/games/CDM/experiments/no_commit/props_recovery_from_3_N5.pctl"...

10 properties:
(1) filter(range, <<[1]>> Pmax=? [ F<=k ((sched=1)&(<<[1]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
(2) filter(range, <<[1, 2]>> Pmax=? [ F<=k ((sched=1|sched=2)&(<<[1, 2]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
(3) filter(range, <<[1, 2, 3]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3)&(<<[1, 2, 3]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
(4) filter(range, <<[1, 2, 3, 4]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4)&(<<[1, 2, 3, 4]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
(5) filter(range, <<[1, 2, 3, 4, 5]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4|sched=5)&(<<[1, 2, 3, 4, 5]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
(6) filter(range, <<[1]>> Pmax=? [ F<=k ((sched=1)&(<<[1]>> R{"ntot1"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
(7) filter(range, <<[1, 2]>> Pmax=? [ F<=k ((sched=1|sched=2)&(<<[1, 2]>> R{"ntot12"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
(8) filter(range, <<[1, 2, 3]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3)&(<<[1, 2, 3]>> R{"ntot123"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
(9) filter(range, <<[1, 2, 3, 4]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4)&(<<[1, 2, 3, 4]>> R{"ntot1234"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
(10) filter(range, <<[1, 2, 3, 4, 5]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4|sched=5)&(<<[1, 2, 3, 4, 5]>> R{"ntot12345"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)

Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0

-------------------------------------------

Building model...

Computing reachable states...
 100032
Reachable states exploration and model construction done in 2.877 secs.
Sorting reachable states list...

Time for model construction: 3.067 seconds.

Type:        SMG

States:      100032 (1 initial)
Transitions: 760430
Choices:     171747
Max/avg:     2/1.72

-------------------------------------------

Model checking: filter(range, <<[1]>> Pmax=? [ F<=k ((sched=1)&(<<[1]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Starting bounded probabilistic reachability...
Bounded probabilistic reachability (maxmin) took 10 iterations and 0.44 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 0.498 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2]>> Pmax=? [ F<=k ((sched=1|sched=2)&(<<[1, 2]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Starting bounded probabilistic reachability...
Bounded probabilistic reachability (maxmin) took 10 iterations and 0.455 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 0.489 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2, 3]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3)&(<<[1, 2, 3]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Starting bounded probabilistic reachability...
Bounded probabilistic reachability (maxmin) took 10 iterations and 0.454 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 0.496 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2, 3, 4]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4)&(<<[1, 2, 3, 4]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Starting bounded probabilistic reachability...
Bounded probabilistic reachability (maxmin) took 10 iterations and 0.517 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 0.555 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2, 3, 4, 5]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4|sched=5)&(<<[1, 2, 3, 4, 5]>> P>0.9 [ F<=10 all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Starting bounded probabilistic reachability...
Bounded probabilistic reachability (maxmin) took 10 iterations and 0.435 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 0.463 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1]>> Pmax=? [ F<=k ((sched=1)&(<<[1]>> R{"ntot1"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.582 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.1 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1692 iterations and 86.939 seconds.
Computed an over-approximation of the solution (in 86 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 2833 iterations and 152.815 seconds.
Expected reachability took 240.367 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 240.462 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2]>> Pmax=? [ F<=k ((sched=1|sched=2)&(<<[1, 2]>> R{"ntot12"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.608 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.1 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1091 iterations and 60.002 seconds.
Computed an over-approximation of the solution (in 60 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 1557 iterations and 85.202 seconds.
Expected reachability took 145.834 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 145.967 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2, 3]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3)&(<<[1, 2, 3]>> R{"ntot123"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.635 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.1 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 760 iterations and 54.927 seconds.
Computed an over-approximation of the solution (in 54 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 871 iterations and 67.615 seconds.
Expected reachability took 123.192 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 123.337 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2, 3, 4]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4)&(<<[1, 2, 3, 4]>> R{"ntot1234"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.76 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.1 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 536 iterations and 42.616 seconds.
Computed an over-approximation of the solution (in 42 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 553 iterations and 43.401 seconds.
Expected reachability took 86.802 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 87.01 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

-------------------------------------------

Model checking: filter(range, <<[1, 2, 3, 4, 5]>> Pmax=? [ F<=k ((sched=1|sched=2|sched=3|sched=4|sched=5)&(<<[1, 2, 3, 4, 5]>> R{"ntot12345"}<50 [ F all_prefer_1 ])) ], all_prefer_3&sched=0)
Model constants: Pexp=0.5,eta=1,gamma=2,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.698 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.1 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 397 iterations and 31.555 seconds.
Computed an over-approximation of the solution (in 31 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 304 iterations and 24.029 seconds.
Expected reachability took 56.309 seconds.

States satisfying filter all_prefer_3&sched=0: 32

Range of values over states satisfying filter: [0.0,0.0]

Time for model checking: 56.568 seconds.

Result: [0.0,0.0] (range of values over states satisfying filter)

