PRISM
=====

Version: 4.0.1.games
Date: Fri Oct 14 22:14:57 BST 2011
Hostname: qavbench.comlab
Command line: prism -ex examples/games/CDM/experiments/no_commit/cdm5032.smg examples/games/CDM/experiments/no_commit/props_runtime_5.pctl -const 'k=0,Pexp=0.5,Q1=1.0,Q2=0.5,Q3=0.25,gamma=5.0,eta=1.0,lambda=1.0'

Parsing model file "examples/games/CDM/experiments/no_commit/cdm5032.smg"...

Parsing properties file "examples/games/CDM/experiments/no_commit/props_runtime_5.pctl"...

5 properties:
(1) <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
(2) <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
(3) <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
(4) <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
(5) <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]

Model constants: Pexp=0.5,eta=1,gamma=5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0

-------------------------------------------

Building model...

Computing reachable states...
 100032
Reachable states exploration and model construction done in 2.805 secs.
Sorting reachable states list...

Time for model construction: 2.995 seconds.

Type:        SMG

States:      100032 (1 initial)
Transitions: 760430
Choices:     171747
Max/avg:     2/1.72

-------------------------------------------

Model checking: <<[0, 1]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.503 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 5461 iterations and 248.617 seconds.
Computed an over-approximation of the solution (in 248 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 2780 iterations and 125.721 seconds.
Expected reachability took 374.865 seconds.

Value in the initial state: 253.7664419550732

Time for model checking: 374.963 seconds.

Result: 253.7664419550732 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.544 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 1593 iterations and 73.562 seconds.
Computed an over-approximation of the solution (in 73 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 934 iterations and 43.072 seconds.
Expected reachability took 117.195 seconds.

Value in the initial state: 62.641858506081164

Time for model checking: 117.217 seconds.

Result: 62.641858506081164 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.566 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 679 iterations and 31.312 seconds.
Computed an over-approximation of the solution (in 31 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 428 iterations and 19.761 seconds.
Expected reachability took 51.652 seconds.

Value in the initial state: 29.511777085684017

Time for model checking: 51.672 seconds.

Result: 29.511777085684017 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 12 iterations and 0.539 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 336 iterations and 15.474 seconds.
Computed an over-approximation of the solution (in 15 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 224 iterations and 10.512 seconds.
Expected reachability took 26.533 seconds.

Value in the initial state: 18.73127149089473

Time for model checking: 26.554 seconds.

Result: 18.73127149089473 (value in the initial state)

-------------------------------------------

Model checking: <<[0, 1, 2, 3, 4, 5]>> R{"runtime"}min=? [ F all_prefer_1 ]
Model constants: Pexp=0.5,eta=1,gamma=5,lambda=1,Q1=1,Q2=0.5,Q3=0.25,k=0
Building reward structure...
Computing rewards...1
Starting expected reachability...
Starting Prob1 (maxmin)...
Prob1 (maxmin) took 11 iterations and 0.499 seconds.
target=192, inf=0, rest=99840
Computing the upper bound where 0.01 is used instead of 0.0
Starting value iteration (minmax)...
Value iteration (minmax) took 173 iterations and 8.33 seconds.
Computed an over-approximation of the solution (in 8 seconds), this will now be used to get the solution
Starting value iteration (minmax)...
Value iteration (minmax) took 126 iterations and 6.112 seconds.
Expected reachability took 14.948 seconds.

Value in the initial state: 13.859314900924025

Time for model checking: 14.968 seconds.

Result: 13.859314900924025 (value in the initial state)

